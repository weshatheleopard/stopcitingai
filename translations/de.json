{
  "lang": "de",
  "locale": "de_DE",
  "language_name": "Deutsch",
  "font": "caveat",
  "ai_models": [
    "ChatGPT",
    "Claude",
    "Gemini",
    "Copilot",
    "Llama",
    "Apple Intelligence",
    "Google AI Mode",
    "Bing AI",
    "Perplexity"
  ],
  "strings": {
    "meta_title": "Hör auf KI zu zitieren",
    "meta_description": "Eine Antwort auf ‘Aber ChatGPT hat gesagt…’",
    "og_image_alt": "Ein Roboter mit Sprechblase auf einem 'Verboten'-Schild, daneben der englische Text 'But ChatGPT said...'",
    "twitter_description": "Eine Antwort auf ‘Aber ChatGPT hat gesagt…'",
    "twitter_image_alt": "Ein Roboter mit Sprechblase auf einem 'Verboten'-Schild, daneben der englische Text 'But ChatGPT said...'",
    "toggle_theme_label": "Zwischen dark- und light-Modus umschalten",
    "but_ai_said": "&ldquo;Aber {ai} hat gesagt…&rdquo;",
    "sent_here_because": "Du wurdest hierhergeschickt, weil du <mark>KI</mark> als Quelle benutzt hast, um etwas zu beweisen.",
    "not_facts_heading": "Antworten von Large Language Modellen wie ChatGPT, Claude, oder Gemini sind keine Fakten.",
    "not_facts_p1": "Sie versuchen die Worte vorherzusagen, die in einem Text am wahrscheinlichsten als nächstes vorkommen.",
    "not_facts_p2": "Sie können überzeugend klingende Informationen erzeugen, aber diese Informationen sind nicht unbedingt korrekt oder zuverlässig.",
    "imagine_heading": "Stell dir vor, eine Person hat tausende Bücher gelesen, aber kann sich nicht erinnern, wo sie was gelesen hat.",
    "good_at_label": "Worin könnte diese Person <mark>gut</mark> sein?",
    "bad_at_label": "Worin könnte diese Person <mark class=\"no\">schlecht</mark> sein?",
    "example_prompt_label": "Prompt-Beispiele",
    "good_prompts": [
      "Hilf mir, Ideen dazu zu brainstormen.",
      "Was sind typische Lösungsansätze dafür?",
      "Fass das zusammen.",
      "Wie kann man das gut ausdrücken?"
    ],
    "bad_prompts": [
      "Sag mir, an welchem Tag das passiert ist.",
      "Was sollte ich hier als nächstes tun?",
      "Ist diese Aussage wahr?",
      "Was ist ein rechtlicher Präzedenzfall dafür?"
    ],
    "might_get_answer": "Sicher, du <em>könntest</em> eine Antwort bekommen, die richtig ist oder einen Rat, der gut ist… Aber an welches “Buch” “erinnert” sich die Person, wenn sie die Antwort gibt? Die Antwort oder der Rat ist eine typische Kombination von Wörtern, kein Fakt.",
    "dont_copy_paste_heading": "Kopiere nicht einfach irgendetwas, was ein Chatbot gesagt hat und schicke es jemandem, als wäre es verlässlich.",
    "dont_copy_paste_p1": "Wenn du das tust, sagst du im Prinzip “Hier sind ein paar Wörter, die oft in einem Satz zusammen vorkommen.”",
    "dont_copy_paste_p2": "Manchmal kann das hilfreich oder aufschlussreich sein. Aber es ist keine <strong>Wahrheit</strong>, und vor allem ist es nicht das letzte Wort in einer Diskussion.",
    "further_reading_heading": "Literaturhinweise:",
    "share_cta": "Schicke das einer Person, die zu dir gesagt hat,",
    "share_cta_quote": "&ldquo;Aber {ai} hat gesagt...&rdquo;",
    "share_button": "Teilen",
    "footer_like_llms": "Ich mag LLMs. Ich mag Machine Learning.",
    "footer_brains_off": "Ich möchte nur nicht sehen, wie kluge Menschen ihr Gehirn abschalten.",
    "footer_about_label": "Mehr über Leo Herzog",
    "github_star_button": "Star on GitHub",
    "github_star_label": "Star on GitHub",
    "github_stars_format": "★ {count} Stars on GitHub",
    "select_language_label": "Sprache auswählen",
    "clipboard_alert": "Link in Zwischenablage kopiert!",
    "share_title": "Hör auf, KI zu zitieren",
    "share_text": "Eine Antwort auf \"Aber ChatGPT hat gesagt...\""
  },
  "further_reading": [
    {
      "text": "KI und die Halluzinationen: Warum sind so viele Antworten falsch?",
      "url": "https://www.heise.de/hintergrund/KI-und-die-Halluzinationen-Warum-sind-so-viele-Antworten-falsch-10962482.html"
    },
    {
      "text": "KI erfindet jede dritte Antwort",
      "url": "https://www.tagesschau.de/wissen/technologie/kuenstliche-intelligenz-fakten-100.html"
    },
    {
      "text": "OpenAI: Why language models hallucinate",
      "url": "https://openai.com/index/why-language-models-hallucinate/"
    },
    {
      "text": "Oxford University: Large Language Models pose risk to science with false answers, says Oxford study",
      "url": "https://www.ox.ac.uk/news/2023-11-20-large-language-models-pose-risk-science-false-answers-says-oxford-study"
    },
    {
      "text": "New York Times: A.I. Is Getting More Powerful, but Its Hallucinations Are Getting Worse",
      "url": "https://www.nytimes.com/2025/05/05/technology/ai-hallucinations-chatgpt-google.html",
      "archived_url": "https://archive.is/CD7Ge"
    },
    {
      "text": "MIT Media Lab: People Overtrust AI-Generated Medical Advice despite Low Accuracy",
      "url": "https://www.media.mit.edu/publications/NEJM-AI-people-overtrust-ai-generated-medical-advice-despite-low-accuracy/"
    },
    {
      "text": "Business Insider: Why AI chatbots hallucinate, according to OpenAI researchers",
      "url": "https://www.businessinsider.com/why-ai-chatbots-hallucinate-openai-chatgpt-anthropic-claude-2025-9"
    },
    {
      "text": "Reuters: AI 'hallucinations' in court papers spell trouble for lawyers",
      "url": "https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/"
    },
    {
      "text": "MIT Technology Review: How AI is introducing errors into courtrooms",
      "url": "https://www.technologyreview.com/2025/05/20/1116823/how-ai-is-introducing-errors-into-courtrooms/"
    },
    {
      "text": "Nature: AI chatbots are sycophants — researchers say it's harming science",
      "url": "https://www.nature.com/articles/d41586-025-03390-0"
    },
    {
      "text": "CNN: Parents of 16-year-old sue OpenAI, claiming ChatGPT advised on his suicide",
      "url": "https://www.cnn.com/2025/08/26/tech/openai-chatgpt-teen-suicide-lawsuit"
    },
    {
      "text": "Financial Times: The ‘hallucinations’ that haunt AI: why chatbots struggle to tell the truth",
      "url": "https://www.ft.com/content/7a4e7eae-f004-486a-987f-4a2e4dbd34fb",
      "archived_url": "https://archive.is/P1Wpc"
    },
    {
      "text": "The Guardian: ‘Sycophantic’ AI chatbots tell users what they want to hear, study shows",
      "url": "https://www.theguardian.com/technology/2025/oct/24/sycophantic-ai-chatbots-tell-users-what-they-want-to-hear-study-shows"
    },
    {
      "text": "PCMag: Vibe Coding Fiasco: AI Agent Goes Rogue, Deletes Company’s Entire Database",
      "url": "https://www.pcmag.com/news/vibe-coding-fiasco-replite-ai-agent-goes-rogue-deletes-company-database"
    },
    {
      "text": "Stephen Wolfram: What Is ChatGPT Doing and Why Does It Work?",
      "url": "https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/"
    }
  ]
}
