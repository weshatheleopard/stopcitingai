{
  "lang": "ru",
  "locale": "ru_RU",
  "language_name": "Русский",
  "font": "caveat",
  "ai_models": [
    "ChatGPT",
    "Claude",
    "Gemini",
    "Copilot",
    "Llama",
    "Apple Intelligence",
    "Google AI Mode",
    "Bing AI",
    "Perplexity"
  ],
  "strings": {
    "meta_title": "Коначайте ссылаться на ИИ",
    "meta_description": "Ответ на «Но ChatGPT сказал…»",
    "og_image_alt": "A robot and speech bubble with a 'ban' circle-slash over it and the text 'But ChatGPT said'",
    "twitter_description": "Ответ на «Но ChatGPT сказал…»",
    "twitter_image_alt": "A robot and speech bubble with a 'ban' circle-slash over it and the text 'But ChatGPT said…'",
    "toggle_theme_label": "Переключить между светлой и тёмной темой",
    "but_ai_said": "«Но {ai} сказал…»",
    "sent_here_because": "Вас отправили сюда, потому что вы, пытаясь что-то доказать, сослались на <mark>ИИ</mark> как на источник.",
    "not_facts_heading": "Ответы больших языковых моделей, таких как ChatGPT, Claude или Gemini, не являются фактами.",
    "not_facts_p1": "Они предсказывают, какие слова с наибольшей вероятностью появятся в тексте следующими.",
    "not_facts_p2": "Они могут генерировать убедительно звучащую информацию — но эта информация может быть неточной или ненадёжной.",
    "imagine_heading": "Представьте человека, который прочитал тысячи книг, но не помнит, где что прочитал.",
    "good_at_label": "В чём они могут быть <mark>хороши</mark>?",
    "bad_at_label": "В чём они могут быть <mark class=\"no\">плохи</mark>?",
    "example_prompt_label": "Пример запроса",
    "good_prompts": [
      "Помоги мне провести мозговой штурм по этой теме",
      "Какие здесь могут быть общие закономерности?",
      "Кратко изложи следующее",
      "Как это получше сформулировать?"
    ],
    "bad_prompts": [
      "Скажи мне, когда произошло это событие",
      "Каковы мои дальнейшие действия после того, как произошло то-то и то-то?",
      "Истинно ли это утверждение?",
      "Какой у этого юридический прецедент?"
    ],
    "might_get_answer": "Конечно, вы <em>можете</em> получить правильный ответ или хороший совет… но какие «книги» он «вспоминает», давая этот ответ? Этот ответ или совет — не факт, а всего лишь распространённое сочетание слов.",
    "dont_copy_paste_heading": "Не копируйте то, что сказал чат-бот, и не отправляйте это кому-то как авторитетное мнение.",
    "dont_copy_paste_p1": "Когда вы это делаете, вы по сути говорите: «вот куча слов, которые часто встречаются вместе в предложении».",
    "dont_copy_paste_p2": "Иногда это может быть полезно или проницательно. Но это не <strong>истина</strong>, и уж точно не последнее слово в каком-либо вопросе.",
    "further_reading_heading": "Дополнительная информация:",
    "share_cta": "Отправьте это тому, кто только что сказал вам,",
    "share_cta_quote": "«Но {ai} сказал...»",
    "share_button": "Поделиться",
    "footer_like_llms": "Мне нравятся большие языковые модели. Мне нравится машинное обучение.",
    "footer_brains_off": "Мне просто не нравится смотреть, как умные люди выключают свой мозг.",
    "footer_about_label": "Подробнее о Leo Herzog",
    "github_star_button": "Лайкни на GitHub",
    "github_star_label": "Лайкни на GitHub",
    "github_stars_format": "★ {count} звёзд на GitHub",
    "select_language_label": "Выберите язык",
    "clipboard_alert": "Ссылка скопирована в буфер обмена!",
    "share_title": "Кончайте ссылаться на ИИ",
    "share_text": "Ответ на «Но ChatGPT сказал…»"
  },
  "further_reading": [
    {
      "text": "OpenAI: Why language models hallucinate",
      "url": "https://openai.com/index/why-language-models-hallucinate/"
    },
    {
      "text": "Oxford University: Large Language Models pose risk to science with false answers, says Oxford study",
      "url": "https://www.ox.ac.uk/news/2023-11-20-large-language-models-pose-risk-science-false-answers-says-oxford-study"
    },
    {
      "text": "New York Times: A.I. Is Getting More Powerful, but Its Hallucinations Are Getting Worse",
      "url": "https://www.nytimes.com/2025/05/05/technology/ai-hallucinations-chatgpt-google.html",
      "archived_url": "https://archive.is/CD7Ge"
    },
    {
      "text": "MIT Media Lab: People Overtrust AI-Generated Medical Advice despite Low Accuracy",
      "url": "https://www.media.mit.edu/publications/NEJM-AI-people-overtrust-ai-generated-medical-advice-despite-low-accuracy/"
    },
    {
      "text": "Business Insider: Why AI chatbots hallucinate, according to OpenAI researchers",
      "url": "https://www.businessinsider.com/why-ai-chatbots-hallucinate-openai-chatgpt-anthropic-claude-2025-9"
    },
    {
      "text": "Reuters: AI 'hallucinations' in court papers spell trouble for lawyers",
      "url": "https://www.reuters.com/technology/artificial-intelligence/ai-hallucinations-court-papers-spell-trouble-lawyers-2025-02-18/"
    },
    {
      "text": "MIT Technology Review: How AI is introducing errors into courtrooms",
      "url": "https://www.technologyreview.com/2025/05/20/1116823/how-ai-is-introducing-errors-into-courtrooms/"
    },
    {
      "text": "Nature: AI chatbots are sycophants — researchers say it's harming science",
      "url": "https://www.nature.com/articles/d41586-025-03390-0"
    },
    {
      "text": "CNN: Parents of 16-year-old sue OpenAI, claiming ChatGPT advised on his suicide",
      "url": "https://www.cnn.com/2025/08/26/tech/openai-chatgpt-teen-suicide-lawsuit"
    },
    {
      "text": "Financial Times: The 'hallucinations' that haunt AI: why chatbots struggle to tell the truth",
      "url": "https://www.ft.com/content/7a4e7eae-f004-486a-987f-4a2e4dbd34fb",
      "archived_url": "https://archive.is/P1Wpc"
    },
    {
      "text": "The Guardian: 'Sycophantic' AI chatbots tell users what they want to hear, study shows",
      "url": "https://www.theguardian.com/technology/2025/oct/24/sycophantic-ai-chatbots-tell-users-what-they-want-to-hear-study-shows"
    },
    {
      "text": "PCMag: Vibe Coding Fiasco: AI Agent Goes Rogue, Deletes Company's Entire Database",
      "url": "https://www.pcmag.com/news/vibe-coding-fiasco-replite-ai-agent-goes-rogue-deletes-company-database"
    },
    {
      "text": "Что делает ChatGPT... и почему это работает?",
      "url": "https://habr.com/ru/articles/739014/"
    }
  ]
}
